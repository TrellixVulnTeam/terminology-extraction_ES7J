{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11980bf3-f80b-4fba-804d-1c5f651d4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hanh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2021-09-14 13:58:17 INFO: Loading these models for language: nl (Dutch):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | alpino  |\n",
      "| pos       | alpino  |\n",
      "| lemma     | alpino  |\n",
      "=======================\n",
      "\n",
      "2021-09-14 13:58:17 INFO: Use device: cpu\n",
      "2021-09-14 13:58:17 INFO: Loading: tokenize\n",
      "2021-09-14 13:58:17 INFO: Loading: pos\n",
      "2021-09-14 13:58:17 INFO: Loading: lemma\n",
      "2021-09-14 13:58:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import stanza\n",
    "# stanza.download('nl')\n",
    "nlp = stanza.Pipeline(lang='nl', processors='tokenize,pos,lemma')\n",
    "\n",
    "def count_freq(preds, gts):\n",
    "    preds_len = [len(x.split(' ')) for x in preds]\n",
    "    gts_len = [len(x.split(' ')) for x in gts]\n",
    "    print(Counter(preds_len))\n",
    "    print(Counter(gts_len))\n",
    "\n",
    "def evaluation_metrics(pred, gt):\n",
    "    TP = len(set(pred) & set(gt)) \n",
    "    FP = len(set(pred)-set(gt))\n",
    "    FN = len(set(gt)-set(pred))\n",
    "    precision = round((TP/(TP+FP))*100, 2)\n",
    "    recall = round((TP/(TP+FN))*100,2)\n",
    "    f1_score = round((2 * precision * recall) / (precision + recall),2)\n",
    "    return precision, recall, f1_score \n",
    "\n",
    "def lemma(li):\n",
    "    new_list = []\n",
    "    for t in li:\n",
    "        doc = nlp(str(t))\n",
    "        doc1 = ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "        doc1 = re.sub('-',' ',doc1)\n",
    "        doc1 = re.sub(' +', ' ',doc1)\n",
    "        new_list.append(doc1)\n",
    "    new_list = [s for s in new_list if len(s) >= 2]\n",
    "    return new_list\n",
    "\n",
    "def get_term_(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        terms = []\n",
    "        term = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label == 'B':\n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "            # Check b_pos = 0 không\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6a7abd-f18b-4b6b-b52b-784a979adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_path ='/Users/hanh/Documents/Github/terminology-extraction/ACTER/nl/'\n",
    "# preds_path = '/Users/hanh/Documents/Github/terminology-extraction/patterns/nl_domains/corp_wind_equi/'\n",
    "preds_path = '/Users/hanh/Documents/Github/terminology-extraction/results/weighted_results/nl/'\n",
    "def term_evaluation(domain_path, preds_path, rule=None):\n",
    "    groundtruth = pd.read_csv(domain_path, sep='\t', engine='python',header=None)\n",
    "    gt = list(groundtruth[0])\n",
    "    predictions = pkl.load(open(preds_path, 'rb'))\n",
    "    preds =  get_term_(predictions)\n",
    "    pred_terms =  set(preds) #- set(stop_words)\n",
    "    pred_terms = [x for x in pred_terms if len(x)>1]\n",
    "    pred_terms = [x.lower().strip() for x in pred_terms]\n",
    "    pred_terms = [re.sub(' -','-', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('- ','-', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('\\(','', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('\\/','', x) for x in pred_terms]\n",
    "    print(evaluation_metrics(pred_terms, gt))\n",
    "    return set(pred_terms), set(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f415c230-418f-4c65-8cdd-72df4859ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63.04, 59.21, 61.07)\n"
     ]
    }
   ],
   "source": [
    "predictions, groundtruth =term_evaluation(domain_path+'htfl/annotations/htfl_nl_terms.ann', preds_path+'ann_weighted_bert_multilingual_uncased.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "171b9a8b-6c26-455d-bf56-7ff1c74b79fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63.29, 58.83, 60.98)\n"
     ]
    }
   ],
   "source": [
    "predictions_, groundtruth_ = term_evaluation(domain_path+'htfl/annotations/htfl_nl_terms_nes.ann',\n",
    "                                             preds_path+'nes_weighted_bert_multilingual_uncased.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0fbbd-ca6c-4cc2-ac44-d0b657bc7c74",
   "metadata": {},
   "source": [
    "## Remove the terms consisting only of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b27eb5-e6c3-4885-9708-c3d292699162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63.23, 59.21, 61.15), (60.18, 60.56, 60.37))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('dutch'))\n",
    "predictions1 =  set(predictions) - set(stop_words)\n",
    "predictions2 =  set(predictions_) - set(stop_words)\n",
    "evaluation_metrics(set(predictions1), groundtruth), evaluation_metrics(set(predictions2), groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdfe60-c197-454c-9b89-7845c2fcab62",
   "metadata": {},
   "source": [
    "## Remove only stopwords at the end of terms consisting of several words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d6bd2cb-decc-42e1-a3bd-e54075ff02d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63.2, 59.21, 61.14), (63.51, 58.83, 61.08))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = [' '.join(x.split()[:-1]) if x.split()[-1] in stop_words else  x for x in predictions]\n",
    "predictions2_ = [' '.join(x.split()[:-1]) if x.split()[-1] in stop_words else  x for x in predictions_]\n",
    "evaluation_metrics(set(predictions2), groundtruth),evaluation_metrics(set(predictions2_), groundtruth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebec2a-6b26-49e0-a63c-6f977a314b6f",
   "metadata": {},
   "source": [
    "## Remove terms consisting of several words but finishing with a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a1893f-12ce-4fc0-9f67-e775c96c44a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63.23, 59.21, 61.15), (60.18, 60.56, 60.37))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3 = [x for x in predictions if x.split()[-1] not in stop_words]\n",
    "predictions3_ = [x for x in predictions_ if x.split()[-1] not in stop_words]\n",
    "evaluation_metrics(set(predictions3), groundtruth), evaluation_metrics(set(predictions3_), groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522335c-7554-480d-91b4-6568e3b699aa",
   "metadata": {},
   "source": [
    "## Export Wrong Prediction Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f545863-348b-409c-948f-04e849179436",
   "metadata": {},
   "outputs": [],
   "source": [
    "htfl = list(set(predictions2) - set(groundtruth))\n",
    "pd.DataFrame(htfl).rename(columns={0:'htfl'}).to_excel('./wrong_prediction/htfl-wrongly-prediction.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
