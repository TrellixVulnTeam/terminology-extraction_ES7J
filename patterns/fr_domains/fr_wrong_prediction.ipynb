{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11980bf3-f80b-4fba-804d-1c5f651d4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hanh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2021-09-14 13:53:03 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-09-14 13:53:03 INFO: Use device: cpu\n",
      "2021-09-14 13:53:03 INFO: Loading: tokenize\n",
      "2021-09-14 13:53:03 INFO: Loading: mwt\n",
      "2021-09-14 13:53:03 INFO: Loading: pos\n",
      "2021-09-14 13:53:03 INFO: Loading: lemma\n",
      "2021-09-14 13:53:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import stanza\n",
    "# stanza.download('fr')\n",
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def count_freq(preds, gts):\n",
    "    preds_len = [len(x.split(' ')) for x in preds]\n",
    "    gts_len = [len(x.split(' ')) for x in gts]\n",
    "    print(Counter(preds_len))\n",
    "    print(Counter(gts_len))\n",
    "\n",
    "def evaluation_metrics(pred, gt):\n",
    "    TP = len(set(pred) & set(gt)) \n",
    "    FP = len(set(pred)-set(gt))\n",
    "    FN = len(set(gt)-set(pred))\n",
    "    precision = round((TP/(TP+FP))*100, 2)\n",
    "    recall = round((TP/(TP+FN))*100,2)\n",
    "    f1_score = round((2 * precision * recall) / (precision + recall),2)\n",
    "    return precision, recall, f1_score \n",
    "\n",
    "def lemma(li):\n",
    "    new_list = []\n",
    "    for t in li:\n",
    "        doc = nlp(str(t))\n",
    "        doc1 = ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "        doc1 = re.sub('-',' ',doc1)\n",
    "        doc1 = re.sub(' +', ' ',doc1)\n",
    "        new_list.append(doc1)\n",
    "    new_list = [s for s in new_list if len(s) >= 2]\n",
    "    return new_list\n",
    "\n",
    "def get_term_(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        terms = []\n",
    "        term = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label == 'B':\n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "            # Check b_pos = 0 không\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6a7abd-f18b-4b6b-b52b-784a979adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_path ='/Users/hanh/Documents/Github/terminology-extraction/ACTER/fr/'\n",
    "# preds_path = '/Users/hanh/Documents/Github/terminology-extraction/patterns/fr_domains/corp_equi_wind/'\n",
    "preds_path = '/Users/hanh/Documents/Github/terminology-extraction/results/weighted_results/fr/'\n",
    "def term_evaluation(domain_path, preds_path, rule=None):\n",
    "    groundtruth = pd.read_csv(domain_path, sep='\t', engine='python',header=None)\n",
    "    gt = list(groundtruth[0])\n",
    "    predictions = pkl.load(open(preds_path, 'rb'))\n",
    "    preds =  get_term_(predictions)\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    pred_terms =  set(preds) - set(stop_words)\n",
    "    pred_terms = [x for x in pred_terms if len(x)>1]\n",
    "    pred_terms = [x.lower().strip() for x in pred_terms]\n",
    "    pred_terms = [re.sub(' -','-', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('- ','-', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('\\(','', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('\\/','', x) for x in pred_terms]\n",
    "    print(evaluation_metrics(pred_terms, gt))\n",
    "    return set(pred_terms), set(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f415c230-418f-4c65-8cdd-72df4859ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53.29, 41.38, 46.59)\n"
     ]
    }
   ],
   "source": [
    "predictions, groundtruth =  term_evaluation(domain_path+'htfl/annotations/htfl_fr_terms.ann', preds_path+'ann_weighted_camembert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0b189f-ce8c-47cb-9201-7d0c71bb930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54.66, 35.8, 43.26)\n"
     ]
    }
   ],
   "source": [
    "predictions_, groundtruth_ =  term_evaluation(domain_path+'htfl/annotations/htfl_fr_terms_nes.ann',\n",
    "                                            preds_path+'nes_weighted_camembert.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0fbbd-ca6c-4cc2-ac44-d0b657bc7c74",
   "metadata": {},
   "source": [
    "## Remove the terms consisting only of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b27eb5-e6c3-4885-9708-c3d292699162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53.29, 41.38, 46.59), (52.6, 36.71, 43.24))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('french'))\n",
    "predictions1 =  set(predictions) - set(stop_words)\n",
    "predictions2 =  set(predictions_) - set(stop_words)\n",
    "evaluation_metrics(set(predictions1), groundtruth), evaluation_metrics(set(predictions2), groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdfe60-c197-454c-9b89-7845c2fcab62",
   "metadata": {},
   "source": [
    "## Remove only stopwords at the end of terms consisting of several words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6bd2cb-decc-42e1-a3bd-e54075ff02d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55.64, 41.88, 47.79), (56.36, 36.02, 43.95))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = [' '.join(x.split()[:-1]) if x.split()[-1] in stop_words else  x for x in predictions]\n",
    "predictions2_ = [' '.join(x.split()[:-1]) if x.split()[-1] in stop_words else  x for x in predictions_]\n",
    "evaluation_metrics(set(predictions2), groundtruth),evaluation_metrics(set(predictions2_), groundtruth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebec2a-6b26-49e0-a63c-6f977a314b6f",
   "metadata": {},
   "source": [
    "## Remove terms consisting of several words but finishing with a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a1893f-12ce-4fc0-9f67-e775c96c44a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56.0, 41.29, 47.53), (54.96, 36.58, 43.92))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3 = [x for x in predictions if x.split()[-1] not in stop_words]\n",
    "predictions3_ = [x for x in predictions_ if x.split()[-1] not in stop_words]\n",
    "evaluation_metrics(set(predictions3), groundtruth), evaluation_metrics(set(predictions3_), groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8dec50-33a4-46c1-8a13-dc3f636a05e1",
   "metadata": {},
   "source": [
    "## Export Wrong Prediction Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85934e1b-77b8-4985-8f9e-45ccf299cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "htfl = list(set(predictions2) - set(groundtruth))\n",
    "pd.DataFrame(htfl).rename(columns={0:'htfl'}).to_excel('./wrong_prediction/htfl-wrongly-prediction.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb0f7c44-5602-4562-afb1-60180fd1d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "htfl_ = list(set(groundtruth)-set(predictions2))\n",
    "pd.DataFrame(htfl_).rename(columns={0:'htfl'}).to_excel('./wrong_prediction/htfl-non-extraction.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd723e6b-cecf-4134-9c98-2e0a7861e551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2228, 744, 1295)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(groundtruth)), len(htfl), len(htfl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170316f6-982f-47b2-96d5-5d1ef192b1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
