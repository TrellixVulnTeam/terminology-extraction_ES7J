{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17242e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hanhtran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2021-05-18 09:19:26 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-05-18 09:19:26 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2021-05-18 09:19:26 INFO: Use device: cpu\n",
      "2021-05-18 09:19:26 INFO: Loading: tokenize\n",
      "2021-05-18 09:19:26 INFO: Loading: pos\n",
      "2021-05-18 09:19:26 INFO: Loading: lemma\n",
      "2021-05-18 09:19:26 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import stanza\n",
    "# stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def count_freq(preds, gts):\n",
    "    preds_len = [len(x.split(' ')) for x in preds]\n",
    "    gts_len = [len(x.split(' ')) for x in gts]\n",
    "    print(Counter(preds_len))\n",
    "    print(Counter(gts_len))\n",
    "\n",
    "def evaluation_metrics(pred, gt):\n",
    "    print(\"Prediction: \", len(set(pred)))\n",
    "    print(\"GT: \",len(set(gt)))\n",
    "    TP = len(set(pred) & set(gt)) \n",
    "    FP = len(set(pred)-set(gt))\n",
    "    FN = len(set(gt)-set(pred))\n",
    "    print(TP,FP,FN)\n",
    "    precision = round((TP/(TP+FP))*100, 2)\n",
    "    recall = round((TP/(TP+FN))*100,2)\n",
    "    f1_score = round((2 * precision * recall) / (precision + recall),2)\n",
    "    return precision, recall, f1_score \n",
    "\n",
    "def lemma(li):\n",
    "    new_list = []\n",
    "    for t in li:\n",
    "        doc = nlp(str(t))\n",
    "        doc1 = ' '.join([word.lemma for sent in doc.sentences for word in sent.words])\n",
    "        doc1 = re.sub('-',' ',doc1)\n",
    "        doc1 = re.sub(' +', ' ',doc1)\n",
    "        new_list.append(doc1)\n",
    "    new_list = [s for s in new_list if len(s) >= 2]\n",
    "    return new_list\n",
    "\n",
    "def get_term_(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        terms = []\n",
    "        term = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label == 'B':\n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "            # Check b_pos = 0 không\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2f26771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        terms = []\n",
    "        term = []\n",
    "        for i, (token, label) in enumerate(zip(tokens, labels)):\n",
    "            if label == 'B': \n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                # Check b_pos = 0 không\n",
    "                if b_pos != 0:\n",
    "                    if (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' '):\n",
    "                        if len(nlp(str(tokens[b_pos - 1])).sentences) > 0:\n",
    "                            b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                            if len(nlp(str(tokens[b_pos])).sentences) > 0:\n",
    "                                c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                                if (c_word.upos == 'NOUN') and (c_word.text != 'None') and (b_word.text != 'None') and ((b_word.upos == 'NOUN') or (b_word.upos == 'ADJ')):\n",
    "                                    terms.append(' '.join([b_word.text] + term))\n",
    "                    if (tokens[i] != '') and (tokens[i] != ' '):\n",
    "                        if len(nlp(str(tokens[i])).sentences) > 0:\n",
    "                            a_word = nlp(str(tokens[i])).sentences[0].words[0]\n",
    "                            if (a_word.text != 'None') and (a_word.upos == 'NOUN'):\n",
    "                                terms.append(' '.join(term + [a_word.text]))\n",
    "                    \n",
    "                    #ADJ NOUN NOUN\n",
    "                    if (tokens[i] != '') and (tokens[i] != ' ') and (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' '):\n",
    "                        if len(nlp(str(tokens[b_pos - 1])).sentences) > 0 and len(nlp(str(tokens[i])).sentences) > 0:\n",
    "                            a_word = nlp(str(tokens[i])).sentences[0].words[0]\n",
    "                            b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                            if len(nlp(str(tokens[b_pos])).sentences) > 0:\n",
    "                                c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                                # Check vị trí b_pos - 1: terms.append()\n",
    "                                if (c_word.text != 'None') and (c_word.upos == 'NOUN') and (b_word.text != 'None') and ((b_word.upos == 'ADJ') and (a_word.upos == 'NOUN')):\n",
    "                                    terms.append(' '.join([b_word.text] + term + [a_word.text]))\n",
    "#                                     print(terms)\n",
    "                    #ADJ ADJ NOUN              \n",
    "                    if (tokens[b_pos - 2] != '') and (tokens[b_pos - 2] != ' ') and (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' '):\n",
    "                        if (len(nlp(str(tokens[b_pos - 2])).sentences) > 0 and len(nlp(str(tokens[b_pos - 1])).sentences) > 0 and len(nlp(str(tokens[b_pos])).sentences) > 0):\n",
    "                            b1_word = nlp(str(tokens[b_pos - 2])).sentences[0].words[0] \n",
    "                            b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                            c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                            # Check vị trí b_pos - 1: terms.append()\n",
    "                            if (c_word.upos == 'NOUN') and (c_word.text != 'None') and (b_word.text != 'None') and (b1_word.text != 'None') and ((b_word.upos == 'ADJ') and (b1_word.upos == 'ADJ')):\n",
    "                                terms.append(' '.join([b1_word.text] +[b_word.text] + term))\n",
    "#                                 print(terms)\n",
    "                    #NOUN ADJ NOUN\n",
    "                    if (tokens[b_pos - 2] != '') and (tokens[b_pos - 2] != ' ') and (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' '):\n",
    "                        if (len(nlp(str(tokens[b_pos - 2])).sentences) > 0 and len(nlp(str(tokens[b_pos - 1])).sentences) > 0 and len(nlp(str(tokens[b_pos])).sentences) > 0):\n",
    "                            b1_word = nlp(str(tokens[b_pos - 2])).sentences[0].words[0] \n",
    "                            b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                            c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                            # Check vị trí b_pos - 1: terms.append()\n",
    "                            if (c_word.upos == 'NOUN') and (c_word.text != 'None') and (b_word.text != 'None') and (b1_word.text != 'None') and ((b_word.upos == 'ADJ') and (b1_word.upos == 'NOUN')):\n",
    "                                terms.append(' '.join([b1_word.text] +[b_word.text] + term))\n",
    "#                                 print(terms)\n",
    "                                \n",
    "                # NOUN NOUN NOUN\n",
    "                if b_pos != 0 and i + 1 < len(tokens):\n",
    "                    if (tokens[i] != '') and (tokens[i] != ' ') and (tokens[i+1] != '') and (tokens[i+1] != ' '):\n",
    "                        if (len(nlp(str(tokens[i])).sentences) > 0 and len(nlp(str(tokens[i+1])).sentences) > 0 and len(nlp(str(tokens[b_pos])).sentences) > 0):\n",
    "                            a1_word = nlp(str(tokens[i+1])).sentences[0].words[0] \n",
    "                            a_word = nlp(str(tokens[i])).sentences[0].words[0]\n",
    "                            c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                            if (c_word.text != 'None') and (c_word.upos == 'NOUN') and (a_word.text != 'None') and (a1_word.text != 'None') and ((a_word.upos == 'NOUN') and (a1_word.upos == 'NOUN')):\n",
    "                                terms.append(' '.join( term + [a_word.text] +[a1_word.text]))\n",
    "#                                 print(terms)\n",
    "                if b_pos != 0:\n",
    "                    # RULE 2\n",
    "                    if (tokens[b_pos-2] != '') and (tokens[b_pos -2] != ' ') and (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' '):\n",
    "                        if len(nlp(str(tokens[b_pos - 1])).sentences) > 0 and len(nlp(str(tokens[b_pos-2])).sentences) > 0:\n",
    "                            b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                            b1_word = nlp(str(tokens[b_pos - 2])).sentences[0].words[0]\n",
    "                            # Check vị trí b_pos - 1: terms.append()\n",
    "                            if  (b_word.text == '-') and (b1_word.text != 'None'):\n",
    "                                terms.append(' '.join([b1_word.text] + [b_word.text] + term))\n",
    "                                print(terms)\n",
    "                    \n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "            # check b_pos - 1\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "085b58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = pd.read_csv('/Users/hanhtran/Documents/terminology-extraction/ACTER/en/htfl/annotations/htfl_en_terms.ann', sep='\t', engine='python',header=None)\n",
    "gt = list(groundtruth[0]) \n",
    "\n",
    "pkl_file = open('../rules/final_preds_sklearn.pkl', 'rb') \n",
    "predictions = pkl.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6b75632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  1674\n",
      "GT:  2361\n",
      "701 973 1660\n",
      "(41.88, 29.69, 34.75)\n",
      "Prediction:  1656\n",
      "GT:  2361\n",
      "704 952 1657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42.51, 29.82, 35.05)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds =  get_term_(predictions)\n",
    "print(evaluation_metrics(preds, gt))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "pred_terms =  set(preds) - set(stop_words)\n",
    "pred_terms = [x for x in pred_terms if len(x)>1]\n",
    "pred_terms = [x.lower().strip() for x in pred_terms]\n",
    "pred_terms = [re.sub(' -','-', x) for x in pred_terms]\n",
    "pred_terms = [re.sub('- ','-', x) for x in pred_terms]\n",
    "pred_terms = [re.sub('\\(','', x) for x in pred_terms]\n",
    "pred_terms = [re.sub('\\/','', x) for x in pred_terms]\n",
    "evaluation_metrics(pred_terms, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a0e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_lem = lemma(pred_terms)\n",
    "# gts_lem = lemma(gt)\n",
    "\n",
    "# print('After lemmatization')\n",
    "# print(len(set(preds_lem)))\n",
    "# print(len(set((gts_lem))))\n",
    "# print(evaluation_metrics(list(set(preds_lem)), list(set(gts_lem))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3082ef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  1674\n",
      "GT:  2361\n",
      "701 973 1660\n",
      "(41.88, 29.69, 34.75)\n",
      "Prediction:  1656\n",
      "GT:  2361\n",
      "704 952 1657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42.51, 29.82, 35.05)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruth = pd.read_csv('/Users/hanhtran/Documents/terminology-extraction/ACTER/en/htfl/annotations/htfl_en_terms.ann', sep='\t', engine='python',header=None)\n",
    "gt = list(groundtruth[0]) \n",
    "\n",
    "pkl_file = open('../rules/final_preds_sklearn.pkl', 'rb') #\n",
    "predictions = pkl.load(pkl_file)\n",
    "preds =  get_term_(predictions)\n",
    "print(evaluation_metrics(preds, gt))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "pred_terms =  set(preds) - set(stop_words)\n",
    "pred_terms = [x for x in pred_terms if len(x)>1]\n",
    "pred_terms = [x.lower().strip() for x in pred_terms]\n",
    "pred_terms = [re.sub(' -','-', x) for x in pred_terms]\n",
    "pred_terms = [re.sub('- ','-', x) for x in pred_terms]\n",
    "pred_terms = [re.sub('\\(','', x) for x in pred_terms]\n",
    "pred_terms = [re.sub('\\/','', x) for x in pred_terms]\n",
    "evaluation_metrics(pred_terms, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debae81c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
