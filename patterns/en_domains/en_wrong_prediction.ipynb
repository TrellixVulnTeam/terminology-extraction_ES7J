{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11980bf3-f80b-4fba-804d-1c5f651d4f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hanh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2021-09-14 14:58:41 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-09-14 14:58:41 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2021-09-14 14:58:41 INFO: Use device: cpu\n",
      "2021-09-14 14:58:41 INFO: Loading: tokenize\n",
      "2021-09-14 14:58:41 INFO: Loading: pos\n",
      "2021-09-14 14:58:41 INFO: Loading: lemma\n",
      "2021-09-14 14:58:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import stanza\n",
    "# stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def evaluation_metrics(pred, gt):\n",
    "    TP = len(set(pred) & set(gt)) \n",
    "    FP = len(set(pred)-set(gt))\n",
    "    FN = len(set(gt)-set(pred))\n",
    "    precision = round((TP/(TP+FP))*100, 2)\n",
    "    recall = round((TP/(TP+FN))*100,2)\n",
    "    f1_score = round((2 * precision * recall) / (precision + recall),2)\n",
    "    return precision, recall, f1_score \n",
    "\n",
    "def get_term_(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        terms = []\n",
    "        term = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label == 'B':\n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "            # Check b_pos = 0 không\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c30890-94c2-4123-99c3-a073db147a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_noun_noun(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "\n",
    "        terms = []\n",
    "        term = []\n",
    "        for i, (token, label) in enumerate(zip(tokens, labels)):\n",
    "            if label == 'B': \n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                if b_pos != 0:\n",
    "                    if (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' ') and (len(nlp(str(tokens[b_pos - 1])).sentences) > 0) and (len(nlp(str(tokens[b_pos])).sentences) > 0):\n",
    "                        b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                        c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                        if (c_word.upos == 'NOUN') and (c_word.text != 'None') and (b_word.text != 'None') and (b_word.upos == 'NOUN'):\n",
    "                            terms.append(' '.join([b_word.text] + term))\n",
    "\n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0ba719-6640-46c4-8cee-3e7fc85d4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_adj_noun(predictions):\n",
    "    all_term = []\n",
    "    for sentence in predictions:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for d in sentence:\n",
    "            tokens.extend(d.keys())\n",
    "            labels.extend(d.values())\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if labels[i] == 'I' and (i == 0 or labels[i - 1] == 'O'):\n",
    "                labels[i] = 'O'\n",
    "        terms = []\n",
    "        term = []\n",
    "        for i, (token, label) in enumerate(zip(tokens, labels)):\n",
    "            if label == 'B': \n",
    "                #Lưu vị trí B\n",
    "                b_pos = i\n",
    "                term = [token]\n",
    "            elif label == 'I':\n",
    "                term.append(token)\n",
    "            elif len(term) > 0:\n",
    "                terms.append(' '.join(term))\n",
    "                if (b_pos != 0) and (tokens[b_pos - 1] != '') and (tokens[b_pos - 1] != ' ') and (len(nlp(str(tokens[b_pos - 1])).sentences) > 0) and (len(nlp(str(tokens[b_pos])).sentences) > 0):\n",
    "                    b_word = nlp(str(tokens[b_pos - 1])).sentences[0].words[0]\n",
    "                    c_word = nlp(str(tokens[b_pos])).sentences[0].words[0]\n",
    "                    if (c_word.upos == 'NOUN') and (c_word.text != 'None') and (b_word.text != 'None') and (b_word.upos == 'ADJ'):\n",
    "                        terms.append(' '.join([b_word.text] + term))\n",
    "                term = []\n",
    "        if len(term) > 0:\n",
    "            terms.append(' '.join(term))\n",
    "        all_term.append(terms)\n",
    "    \n",
    "    final_terms = []\n",
    "    for i in all_term:\n",
    "        final_terms.extend(i)\n",
    "\n",
    "    final_terms = [x.lower().strip() for x in final_terms]\n",
    "    return final_terms    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6a7abd-f18b-4b6b-b52b-784a979adb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_path ='/Users/hanh/Documents/Github/terminology-extraction/ACTER/en/'\n",
    "preds_path = '/Users/hanh/Documents/Github/terminology-extraction/patterns/en_domains/corp_equi_wind/'\n",
    "def term_evaluation(domain_path, preds_path, rule=None):\n",
    "    groundtruth = pd.read_csv(domain_path, sep='\t', engine='python',header=None)\n",
    "    gt = list(groundtruth[0])\n",
    "    predictions = pkl.load(open(preds_path, 'rb'))\n",
    "    preds =  get_term_(predictions)\n",
    "    preds1 =  get_term_noun_noun(predictions)\n",
    "    preds2 =  get_term_adj_noun(predictions)\n",
    "    preds = preds + preds1 + preds2 \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pred_terms =  set(preds) # - set(stop_words)\n",
    "    pred_terms = [x for x in pred_terms if len(x)>1]\n",
    "#     pred_terms = [x.lower().strip() for x in pred_terms]\n",
    "    pred_terms = [re.sub(' -','-', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('- ','-', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('\\(','', x) for x in pred_terms]\n",
    "    pred_terms = [re.sub('\\/','', x) for x in pred_terms]\n",
    "    print(evaluation_metrics(pred_terms, gt))\n",
    "    return set(pred_terms), set(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d60a32c-a705-4f39-a08f-7f5f3e4a8192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38.78, 37.99, 38.38)\n",
      "(39.91, 43.52, 41.64)\n"
     ]
    }
   ],
   "source": [
    "predictions, groundtruth = term_evaluation(domain_path+'htfl/annotations/htfl_en_terms.ann', preds_path+'ann_xlnet_htfl.pkl')\n",
    "predictions_, groundtruth_ = term_evaluation(domain_path+'htfl/annotations/htfl_en_terms_nes.ann', preds_path+'nes_xlnet_htfl.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7725e5be-8a0b-48ab-bd5c-e6b9a9b664dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38.78, 37.99, 38.38)\n",
      "(39.91, 43.52, 41.64)\n"
     ]
    }
   ],
   "source": [
    "predictions, groundtruth = term_evaluation(domain_path+'htfl/annotations/htfl_en_terms.ann', preds_path+'ann_xlnet_htfl.pkl')\n",
    "predictions_, groundtruth_ = term_evaluation(domain_path+'htfl/annotations/htfl_en_terms_nes.ann', preds_path+'nes_xlnet_htfl.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85879db-10b7-4947-976f-ae33201a2e2e",
   "metadata": {},
   "source": [
    "## Remove the terms consisting only of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81e7f6a-c32c-4f68-9836-e7021bf5d599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.78, 37.99, 38.38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "predictions1 =  set(predictions) - set(stop_words)\n",
    "evaluation_metrics(set(predictions), groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e7272b-fe8f-4a69-aa7c-76802803c2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.9, 37.91, 38.4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics(set(predictions1), groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05e12ee-1c43-4f4a-be55-12c7a78ca066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.91, 43.52, 41.64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1_ =  set(predictions_) - set(stop_words)\n",
    "evaluation_metrics(set(predictions_), groundtruth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc889d8-2ddb-49bc-8bea-0e613a6ac94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.03, 43.4, 41.65)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics(set(predictions1_), groundtruth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861860b-5003-4009-a770-3fce7cb3c825",
   "metadata": {},
   "source": [
    "## Remove only stopwords at the end of terms consisting of several words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d8fa7f9-c9a4-402c-b5d6-129d33cca39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.96, 37.95, 38.45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = [' '.join(x.split()[:-1]) if x.split()[-1] in stop_words else  x for x in predictions]\n",
    "evaluation_metrics(set(predictions2), groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff9cb81-0716-45d6-852a-9ee4e1553ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2300, 2313)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(predictions2)), len(set(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205851d1-38ea-4351-9782-a3a85fb75581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.04, 43.4, 41.65)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2_ = [' '.join(x.split()[:-1]) if x.split()[-1] in stop_words else  x for x in predictions_]\n",
    "evaluation_metrics(set(predictions2_), groundtruth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43ac61-11fe-49a2-93bb-4ed6dc08e8d1",
   "metadata": {},
   "source": [
    "## Remove terms consisting of several words but finishing with a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc93f05-72e1-4d19-b2c7-a10860302691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.08, 37.91, 38.49)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3 = [x for x in predictions if x.split()[-1] not in stop_words]\n",
    "evaluation_metrics(set(predictions3), groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c98ed3f8-71d0-467f-9d70-1dc463c54338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.41, 44.3, 40.56)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3_ = [x for x in predictions_ if x.split()[-1] not in stop_words]\n",
    "evaluation_metrics(set(predictions3_), groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3984caa6-b58c-4548-8a9d-57d6fd37db08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 2290)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions2), len(predictions3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2f409-2870-407e-b9fa-dc3a04fa2032",
   "metadata": {},
   "source": [
    "## Export Wrong Prediction Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f415c230-418f-4c65-8cdd-72df4859ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, groundtruth = term_evaluation(domain_path+'htfl/annotations/htfl_en_terms.ann', preds_path+'ann_xlnet_htfl.pkl')\n",
    "# htfl = list(predictions - groundtruth)\n",
    "# pd.DataFrame(htfl).rename(columns={0:'htfl'}).to_csv('./wrong_prediction/htfl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de70ead-6ccd-4bf7-9753-cbb7a39c9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, groundtruth = term_evaluation(domain_path+'equi/annotations/equi_en_terms.ann', preds_path+'ann_xlnet_equi.pkl')\n",
    "# equi = list(predictions - groundtruth)\n",
    "# pd.DataFrame(equi).rename(columns={0:'equi'}).to_csv('./wrong_prediction/equi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dcb4936-f4ae-443e-9270-ca1a35471d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, groundtruth = term_evaluation(domain_path+'wind/annotations/wind_en_terms.ann', preds_path+'ann_xlnet_wind.pkl')\n",
    "# wind = list(predictions - groundtruth)\n",
    "# pd.DataFrame(wind).rename(columns={0:'wind'}).to_csv('./wrong_prediction/wind.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d074419a-1c6d-413e-8791-796d0d078def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, groundtruth = term_evaluation(domain_path+'corp/annotations/corp_en_terms.ann', preds_path+'ann_xlnet_corp.pkl')\n",
    "# corp = list(predictions - groundtruth)\n",
    "# pd.DataFrame(corp).rename(columns={0:'corp'}).to_csv('./wrong_prediction/corp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f19be2-1c88-449b-88a6-7be0537617c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = htfl + equi + wind + corp\n",
    "# len(final), len(set(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3c840ab-5c95-4316-904a-d9e1ac9f5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong_predict = pd.read_excel('./wrong_prediction/wrongly-predicted.xlsx')\n",
    "# wrong_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633be784-8557-493d-91aa-609ce41ea4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
